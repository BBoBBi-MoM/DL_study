{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset,DataLoader\n",
    "\n",
    "import torchtext\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "\n",
    "import torchdata\n",
    "\n",
    "from konlpy.tag import Okt \n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_table('ratings_train.txt')\n",
    "test_df = pd.read_table('ratings_test.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>document</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9976970</td>\n",
       "      <td>아 더빙.. 진짜 짜증나네요 목소리</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3819312</td>\n",
       "      <td>흠...포스터보고 초딩영화줄....오버연기조차 가볍지 않구나</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10265843</td>\n",
       "      <td>너무재밓었다그래서보는것을추천한다</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9045019</td>\n",
       "      <td>교도소 이야기구먼 ..솔직히 재미는 없다..평점 조정</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6483659</td>\n",
       "      <td>사이몬페그의 익살스런 연기가 돋보였던 영화!스파이더맨에서 늙어보이기만 했던 커스틴 ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         id                                           document  label\n",
       "0   9976970                                아 더빙.. 진짜 짜증나네요 목소리      0\n",
       "1   3819312                  흠...포스터보고 초딩영화줄....오버연기조차 가볍지 않구나      1\n",
       "2  10265843                                  너무재밓었다그래서보는것을추천한다      0\n",
       "3   9045019                      교도소 이야기구먼 ..솔직히 재미는 없다..평점 조정      0\n",
       "4   6483659  사이몬페그의 익살스런 연기가 돋보였던 영화!스파이더맨에서 늙어보이기만 했던 커스틴 ...      1"
      ]
     },
     "execution_count": 259,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>document</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6270596</td>\n",
       "      <td>굳 ㅋ</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9274899</td>\n",
       "      <td>GDNTOPCLASSINTHECLUB</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8544678</td>\n",
       "      <td>뭐야 이 평점들은.... 나쁘진 않지만 10점 짜리는 더더욱 아니잖아</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6825595</td>\n",
       "      <td>지루하지는 않은데 완전 막장임... 돈주고 보기에는....</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6723715</td>\n",
       "      <td>3D만 아니었어도 별 다섯 개 줬을텐데.. 왜 3D로 나와서 제 심기를 불편하게 하죠??</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id                                           document  label\n",
       "0  6270596                                                굳 ㅋ      1\n",
       "1  9274899                               GDNTOPCLASSINTHECLUB      0\n",
       "2  8544678             뭐야 이 평점들은.... 나쁘진 않지만 10점 짜리는 더더욱 아니잖아      0\n",
       "3  6825595                   지루하지는 않은데 완전 막장임... 돈주고 보기에는....      0\n",
       "4  6723715  3D만 아니었어도 별 다섯 개 줬을텐데.. 왜 3D로 나와서 제 심기를 불편하게 하죠??      0"
      ]
     },
     "execution_count": 260,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = train_df.drop(['id'],axis=1)\n",
    "test_df = test_df.drop(['id'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['document'] = train_df['document'].str.replace(\"[^ㄱ-ㅎㅏ-ㅣ가-힣 ]\",\"\",regex=True)\n",
    "test_df['document'] = test_df['document'].str.replace(\"[^ㄱ-ㅎㅏ-ㅣ가-힣 ]\",\"\",regex=True)\n",
    "train_df.drop(train_df[train_df['document']==''].index, inplace=True)\n",
    "test_df.drop(test_df[test_df['document']==''].index, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train 결측치 정보\n",
      "document    5\n",
      "label       0\n",
      "dtype: int64\n",
      "\n",
      " train결측치 정보\n",
      "document    0\n",
      "label       0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print('train 결측치 정보')\n",
    "print(train_df.isna().sum())\n",
    "train_df.dropna(inplace=True)\n",
    "print('\\n train결측치 정보')\n",
    "print(train_df.isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test 결측치 정보\n",
      "document    3\n",
      "label       0\n",
      "dtype: int64\n",
      "\n",
      "test 결측치 정보\n",
      "document    0\n",
      "label       0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print('test 결측치 정보')\n",
    "print(test_df.isna().sum())\n",
    "test_df.dropna(inplace=True)\n",
    "print('\\ntest 결측치 정보')\n",
    "print(test_df.isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_array,val_array = train_df.iloc[:int(len(train_df)*0.8),:].values, train_df.iloc[int(len(train_df)*0.8):,:].values\n",
    "test_array = test_df.values"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#### 불용어\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['아 더빙 진짜 짜증나네요 목소리', 0],\n",
       "       ['흠포스터보고 초딩영화줄오버연기조차 가볍지 않구나', 1],\n",
       "       ['너무재밓었다그래서보는것을추천한다', 0],\n",
       "       ...,\n",
       "       ['와 진짜 내가 왠만해서 진짜 씹노잼이다와 이거 점주는인간들 머지 도대체 영화를 볼줄아는건가 ㅋㅋ', 0],\n",
       "       ['다빈씨 하늘에선 행복 하시길', 1],\n",
       "       ['편 본사람은 완전실망비디오용도 고려 울아거들도 무표정', 0]], dtype=object)"
      ]
     },
     "execution_count": 266,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9a17be5148a548c6a536bbb97b23de60",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/119348 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer = Okt()\n",
    "\n",
    "stop_words_file = open('stopwords_kor.txt','r')\n",
    "stop_words_list = stop_words_file.read().split('\\n')\n",
    "\n",
    "def yield_tokenizer(array):\n",
    "    for sentence,_ in tqdm(array):\n",
    "        sentence = tokenizer.morphs(sentence)\n",
    "        sentence = [word for word in sentence if not word in stop_words_list]\n",
    "        yield sentence\n",
    "\n",
    "vocab = build_vocab_from_iterator(iterator=yield_tokenizer(train_array), \n",
    "                                  specials=['<unk>', '<pad>'], \n",
    "                                  special_first=True, \n",
    "                                  max_tokens=5000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_pipeline(sentence:str):\n",
    "    output = list()\n",
    "    sentence = tokenizer.morphs(sentence)\n",
    "    for word in sentence:\n",
    "        if word in vocab:\n",
    "            output.append(word)\n",
    "        else:\n",
    "            output.append('<unk>')\n",
    "    return vocab(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_batch(batch):\n",
    "    label_list, text_list, offsets = [], [], [0]\n",
    "    for text,label in batch:\n",
    "        label_list.append(label)\n",
    "        processed_text = text_pipeline(text)\n",
    "        processed_text = torch.tensor(processed_text, dtype=torch.int64)\n",
    "        text_list.append(processed_text)\n",
    "        offsets.append(processed_text.size(0))\n",
    "    label_list = torch.tensor(label_list, dtype=torch.int64)\n",
    "    offsets = torch.tensor(offsets[:-1]).cumsum(dim=0)\n",
    "    text_list = torch.cat(text_list)\n",
    "    return label_list.to(device), text_list.to(device), offsets.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_length = len(vocab)\n",
    "BATCH_SIZE = 64\n",
    "train_loader = DataLoader(train_array, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_batch)\n",
    "val_loader = DataLoader(val_array, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_batch)\n",
    "test_loader = DataLoader(test_array, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, hidden_dim, output_dim, n_layers, dropout):\n",
    "        super().__init__()\n",
    "        self.embed = nn.EmbeddingBag(vocab_size, embed_dim, sparse=False)\n",
    "        self.rnn = nn.RNN(input_size = embed_dim,\n",
    "                          hidden_size = hidden_dim,\n",
    "                          num_layers = n_layers,\n",
    "                          dropout=dropout)\n",
    "        \n",
    "        self.fc_layer1 = nn.Sequential(\n",
    "            nn.Linear(256,256),\n",
    "            nn.ReLU()\n",
    "            )\n",
    "        \n",
    "        self.drop = nn.Dropout(dropout)\n",
    "        \n",
    "        self.output_layer = nn.Sequential(\n",
    "            nn.Linear(256, output_dim),\n",
    "            nn.Sigmoid()\n",
    "            )\n",
    "\n",
    "    def forward(self,text,offsests):\n",
    "        embedded = self.embed(text,offsests)\n",
    "        output, hidden = self.rnn(embedded)\n",
    "        output = output.view(output.shape[0], -1)\n",
    "        output = self.fc_layer1(output)\n",
    "        output = self.output_layer(output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RNNClassifier(vocab_size=vocab_length,\n",
    "                      embed_dim=64,\n",
    "                      hidden_dim=256,\n",
    "                      output_dim=2,\n",
    "                      n_layers=2,\n",
    "                      dropout=0.5).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 5\n",
    "LEARNING_RATE = 1e-5\n",
    "\n",
    "loss_func = nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef543554c206462593809bb41b7bef44",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "50d51f4cc1d84bdbb5cd17afaf3ad652",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1865 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc1f347ee94b473c9a13647bf97f2e32",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/467 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[EPOCH: 21/5] [TRAIN LOSS:0.37378] [TRAIN ACCURACY:83.22%] [VAL LOSS:0.40284] [VAL ACCURACY: 81.93%]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "57d3c7f9c65a40c2a61403ff09425087",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1865 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c7a6f8026dbe44ec95d91b6bf9b59156",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/467 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[EPOCH: 22/5] [TRAIN LOSS:0.37284] [TRAIN ACCURACY:83.35%] [VAL LOSS:0.40311] [VAL ACCURACY: 81.89%]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d2c05ae071c949759ac5c0bf21b8c798",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1865 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "33919adbb308484280882bc3481a24d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/467 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[EPOCH: 23/5] [TRAIN LOSS:0.37226] [TRAIN ACCURACY:83.38%] [VAL LOSS:0.40309] [VAL ACCURACY: 81.91%]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "57fc5d7d1d554bee8bfefba7f7b4e67d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1865 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d87ff09b856246af8e4a4f3b2b63309b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/467 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[EPOCH: 24/5] [TRAIN LOSS:0.37162] [TRAIN ACCURACY:83.43%] [VAL LOSS:0.40276] [VAL ACCURACY: 81.91%]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9cc956c64b794ea9990088516fddf8ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1865 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d7937044e334f8fa4743fd091649cd8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/467 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[EPOCH: 25/5] [TRAIN LOSS:0.37097] [TRAIN ACCURACY:83.42%] [VAL LOSS:0.40273] [VAL ACCURACY: 81.94%]\n"
     ]
    }
   ],
   "source": [
    "for epoch in tqdm(range(20,20+EPOCHS)):\n",
    "    model.train()\n",
    "    epoch_avg_loss = 0.0\n",
    "    epoch_correct = 0\n",
    "\n",
    "    for labels, texts, offsets in tqdm(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(texts, offsets)\n",
    "        epoch_correct += (outputs.argmax(-1)==labels).sum().item()\n",
    "        labels = nn.functional.one_hot(labels)\n",
    "        labels = labels.type(torch.float32)\n",
    "        batch_loss = loss_func(outputs,labels)\n",
    "        epoch_avg_loss += batch_loss.item()/len(train_loader)\n",
    "        batch_loss.backward()\n",
    "        optimizer.step()\n",
    "    epoch_accuracy = epoch_correct/len(train_array)*100\n",
    "    \n",
    "    # eval\n",
    "    model.eval()\n",
    "    val_avg_loss = 0.0\n",
    "    val_correct = 0\n",
    "    with torch.no_grad():\n",
    "        for val_labels, val_texts, val_offsets in tqdm(val_loader):\n",
    "            val_outputs = model(val_texts,val_offsets)\n",
    "            val_correct += (val_outputs.argmax(-1)==val_labels).sum().item()\n",
    "            val_labels = nn.functional.one_hot(val_labels)\n",
    "            val_labels = val_labels.type(torch.float32)\n",
    "            batch_loss = loss_func(val_outputs,val_labels)\n",
    "            val_batch_loss = loss_func(val_outputs,val_labels)\n",
    "            val_avg_loss += val_batch_loss.item()/len(val_loader)\n",
    "        val_accuracy = val_correct/len(val_array)*100\n",
    "\n",
    "    print(f'[EPOCH: {epoch+1:2}/{EPOCHS}] [TRAIN LOSS:{epoch_avg_loss:.5f}] [TRAIN ACCURACY:{epoch_accuracy:.2f}%] [VAL LOSS:{val_avg_loss:.5f}] [VAL ACCURACY: {val_accuracy:.2f}%]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "44b76ef933c04a44b9de612666c78f92",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/777 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TEST LOSS:0.40482] [TEST ACCURACY: 81.68%]\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "test_avg_loss = 0.0\n",
    "test_correct = 0\n",
    "with torch.no_grad():\n",
    "    for test_labels, test_texts, test_offsets in tqdm(test_loader):\n",
    "        test_outputs = model(test_texts, test_offsets)\n",
    "        test_correct += (test_outputs.argmax(-1)==test_labels).sum().item()\n",
    "        test_labels = nn.functional.one_hot(test_labels)\n",
    "        test_labels = test_labels.type(torch.float32)\n",
    "        batch_loss = loss_func(test_outputs, test_labels)\n",
    "        test_batch_loss = loss_func(test_outputs, test_labels)\n",
    "        test_avg_loss += test_batch_loss.item()/len(test_loader)\n",
    "    test_accuracy = test_correct/len(test_array)*100\n",
    "    print(f'[TEST LOSS:{test_avg_loss:.5f}] [TEST ACCURACY: {test_accuracy:.2f}%]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GRUClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, hidden_dim, output_dim, n_layers, dropout):\n",
    "        super().__init__()\n",
    "        self.embed = nn.EmbeddingBag(vocab_size, embed_dim, sparse=False)\n",
    "        self.rnn = nn.GRU(input_size = embed_dim,\n",
    "                          hidden_size = hidden_dim,\n",
    "                          num_layers = n_layers,\n",
    "                          dropout=dropout)\n",
    "        \n",
    "        self.fc_layer1 = nn.Sequential(\n",
    "            nn.Linear(256,256),\n",
    "            nn.ReLU()\n",
    "            )\n",
    "        \n",
    "        self.drop = nn.Dropout(dropout)\n",
    "        \n",
    "        self.output_layer = nn.Sequential(\n",
    "            nn.Linear(256, output_dim),\n",
    "            nn.Sigmoid()\n",
    "            )\n",
    "\n",
    "    def forward(self,text,offsests):\n",
    "        embedded = self.embed(text,offsests)\n",
    "        output, hidden = self.rnn(embedded)\n",
    "        output = output.view(output.shape[0], -1)\n",
    "        output = self.fc_layer1(output)\n",
    "        output = self.output_layer(output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = GRUClassifier(vocab_size=vocab_length,\n",
    "                      embed_dim=64,\n",
    "                      hidden_dim=256,\n",
    "                      output_dim=2,\n",
    "                      n_layers=2,\n",
    "                      dropout=0.5).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 5\n",
    "LEARNING_RATE = 1e-4\n",
    "\n",
    "loss_func1 = nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(model2.parameters(), lr=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "97a0f359b6a54425bace5d96ae56f79e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c9e27f5541b474bbdf81acf01fa17f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1865 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "537ffc32cff5456fb7fd7b23f6e006ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/467 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[EPOCH:  1/30] [TRAIN LOSS:0.65994] [TRAIN ACCURACY:58.79%] [VAL LOSS:0.60957] [VAL ACCURACY: 66.68%]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "98ef324bcc0245f0a17655c370d60621",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1865 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f1b8c6bbda84dcdab89822e93ff3081",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/467 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[291], line 23\u001b[0m\n\u001b[0;32m     21\u001b[0m val_correct \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[0;32m     22\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[1;32m---> 23\u001b[0m     \u001b[39mfor\u001b[39;00m val_labels, val_texts, val_offsets \u001b[39min\u001b[39;00m tqdm(val_loader):\n\u001b[0;32m     24\u001b[0m         val_outputs \u001b[39m=\u001b[39m model2(val_texts,val_offsets)\n\u001b[0;32m     25\u001b[0m         val_correct \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m (val_outputs\u001b[39m.\u001b[39margmax(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\u001b[39m==\u001b[39mval_labels)\u001b[39m.\u001b[39msum()\u001b[39m.\u001b[39mitem()\n",
      "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\envs\\nlp\\lib\\site-packages\\tqdm\\notebook.py:254\u001b[0m, in \u001b[0;36mtqdm_notebook.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    252\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m    253\u001b[0m     it \u001b[39m=\u001b[39m \u001b[39msuper\u001b[39m(tqdm_notebook, \u001b[39mself\u001b[39m)\u001b[39m.\u001b[39m\u001b[39m__iter__\u001b[39m()\n\u001b[1;32m--> 254\u001b[0m     \u001b[39mfor\u001b[39;00m obj \u001b[39min\u001b[39;00m it:\n\u001b[0;32m    255\u001b[0m         \u001b[39m# return super(tqdm...) will not catch exception\u001b[39;00m\n\u001b[0;32m    256\u001b[0m         \u001b[39myield\u001b[39;00m obj\n\u001b[0;32m    257\u001b[0m \u001b[39m# NB: except ... [ as ...] breaks IPython async KeyboardInterrupt\u001b[39;00m\n",
      "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\envs\\nlp\\lib\\site-packages\\tqdm\\std.py:1178\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1175\u001b[0m time \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_time\n\u001b[0;32m   1177\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m-> 1178\u001b[0m     \u001b[39mfor\u001b[39;00m obj \u001b[39min\u001b[39;00m iterable:\n\u001b[0;32m   1179\u001b[0m         \u001b[39myield\u001b[39;00m obj\n\u001b[0;32m   1180\u001b[0m         \u001b[39m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[0;32m   1181\u001b[0m         \u001b[39m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\envs\\nlp\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:634\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    631\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    632\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    633\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 634\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[0;32m    635\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m    636\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[0;32m    637\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[0;32m    638\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\envs\\nlp\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:678\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    676\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_next_data\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m    677\u001b[0m     index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_index()  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 678\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dataset_fetcher\u001b[39m.\u001b[39;49mfetch(index)  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    679\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory:\n\u001b[0;32m    680\u001b[0m         data \u001b[39m=\u001b[39m _utils\u001b[39m.\u001b[39mpin_memory\u001b[39m.\u001b[39mpin_memory(data, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\envs\\nlp\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:54\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n\u001b[1;32m---> 54\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcollate_fn(data)\n",
      "Cell \u001b[1;32mIn[269], line 5\u001b[0m, in \u001b[0;36mcollate_batch\u001b[1;34m(batch)\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[39mfor\u001b[39;00m text,label \u001b[39min\u001b[39;00m batch:\n\u001b[0;32m      4\u001b[0m     label_list\u001b[39m.\u001b[39mappend(label)\n\u001b[1;32m----> 5\u001b[0m     processed_text \u001b[39m=\u001b[39m text_pipeline(text)\n\u001b[0;32m      6\u001b[0m     processed_text \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mtensor(processed_text, dtype\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mint64)\n\u001b[0;32m      7\u001b[0m     text_list\u001b[39m.\u001b[39mappend(processed_text)\n",
      "Cell \u001b[1;32mIn[268], line 3\u001b[0m, in \u001b[0;36mtext_pipeline\u001b[1;34m(sentence)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mtext_pipeline\u001b[39m(sentence:\u001b[39mstr\u001b[39m):\n\u001b[0;32m      2\u001b[0m     output \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m()\n\u001b[1;32m----> 3\u001b[0m     sentence \u001b[39m=\u001b[39m tokenizer\u001b[39m.\u001b[39;49mmorphs(sentence)\n\u001b[0;32m      4\u001b[0m     \u001b[39mfor\u001b[39;00m word \u001b[39min\u001b[39;00m sentence:\n\u001b[0;32m      5\u001b[0m         \u001b[39mif\u001b[39;00m word \u001b[39min\u001b[39;00m vocab:\n",
      "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\envs\\nlp\\lib\\site-packages\\konlpy\\tag\\_okt.py:89\u001b[0m, in \u001b[0;36mOkt.morphs\u001b[1;34m(self, phrase, norm, stem)\u001b[0m\n\u001b[0;32m     86\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mmorphs\u001b[39m(\u001b[39mself\u001b[39m, phrase, norm\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, stem\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m):\n\u001b[0;32m     87\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Parse phrase to morphemes.\"\"\"\u001b[39;00m\n\u001b[1;32m---> 89\u001b[0m     \u001b[39mreturn\u001b[39;00m [s \u001b[39mfor\u001b[39;00m s, t \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpos(phrase, norm\u001b[39m=\u001b[39;49mnorm, stem\u001b[39m=\u001b[39;49mstem)]\n",
      "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\envs\\nlp\\lib\\site-packages\\konlpy\\tag\\_okt.py:71\u001b[0m, in \u001b[0;36mOkt.pos\u001b[1;34m(self, phrase, norm, stem, join)\u001b[0m\n\u001b[0;32m     59\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"POS tagger.\u001b[39;00m\n\u001b[0;32m     60\u001b[0m \u001b[39mIn contrast to other classes in this subpackage,\u001b[39;00m\n\u001b[0;32m     61\u001b[0m \u001b[39mthis POS tagger doesn't have a `flatten` option,\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     67\u001b[0m \u001b[39m:param join: If True, returns joined sets of morph and tag.\u001b[39;00m\n\u001b[0;32m     68\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m     69\u001b[0m validate_phrase_inputs(phrase)\n\u001b[1;32m---> 71\u001b[0m tokens \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mjki\u001b[39m.\u001b[39;49mtokenize(\n\u001b[0;32m     72\u001b[0m             phrase,\n\u001b[0;32m     73\u001b[0m             jpype\u001b[39m.\u001b[39;49mjava\u001b[39m.\u001b[39;49mlang\u001b[39m.\u001b[39;49mBoolean(norm),\n\u001b[0;32m     74\u001b[0m             jpype\u001b[39m.\u001b[39;49mjava\u001b[39m.\u001b[39;49mlang\u001b[39m.\u001b[39;49mBoolean(stem))\u001b[39m.\u001b[39mtoArray()\n\u001b[0;32m     75\u001b[0m \u001b[39mif\u001b[39;00m join:\n\u001b[0;32m     76\u001b[0m     \u001b[39mreturn\u001b[39;00m [t \u001b[39mfor\u001b[39;00m t \u001b[39min\u001b[39;00m tokens]\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for epoch in tqdm(range(30)):\n",
    "    model2.train()\n",
    "    epoch_avg_loss = 0.0\n",
    "    epoch_correct = 0\n",
    "\n",
    "    for labels, texts, offsets in tqdm(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model2(texts, offsets)\n",
    "        epoch_correct += (outputs.argmax(-1)==labels).sum().item()\n",
    "        labels = nn.functional.one_hot(labels)\n",
    "        labels = labels.type(torch.float32)\n",
    "        batch_loss = loss_func1(outputs,labels)\n",
    "        epoch_avg_loss += batch_loss.item()/len(train_loader)\n",
    "        batch_loss.backward()\n",
    "        optimizer.step()\n",
    "    epoch_accuracy = epoch_correct/len(train_array)*100\n",
    "    \n",
    "    # eval\n",
    "    model2.eval()\n",
    "    val_avg_loss = 0.0\n",
    "    val_correct = 0\n",
    "    with torch.no_grad():\n",
    "        for val_labels, val_texts, val_offsets in tqdm(val_loader):\n",
    "            val_outputs = model2(val_texts,val_offsets)\n",
    "            val_correct += (val_outputs.argmax(-1)==val_labels).sum().item()\n",
    "            val_labels = nn.functional.one_hot(val_labels)\n",
    "            val_labels = val_labels.type(torch.float32)\n",
    "            batch_loss = loss_func1(val_outputs,val_labels)\n",
    "            val_batch_loss = loss_func(val_outputs,val_labels)\n",
    "            val_avg_loss += val_batch_loss.item()/len(val_loader)\n",
    "        val_accuracy = val_correct/len(val_array)*100\n",
    "\n",
    "    print(f'[EPOCH: {epoch+1:2}/{30}] [TRAIN LOSS:{epoch_avg_loss:.5f}] [TRAIN ACCURACY:{epoch_accuracy:.2f}%] [VAL LOSS:{val_avg_loss:.5f}] [VAL ACCURACY: {val_accuracy:.2f}%]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "98e2bfb919ff5917b98b90a86d4547b4b7593f6b54c8d2e707cb1748df7f4086"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
