{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch import Tensor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PatchEmbedding(nn.Module):\n",
    "    def __init__(self,\n",
    "                 in_channel,\n",
    "                 patch_size,\n",
    "                 emb_dim=None):\n",
    "        super().__init__()\n",
    "\n",
    "        d = in_channel * (patch_size ** 2)\n",
    "\n",
    "        if emb_dim is None:\n",
    "            emb_dim = d\n",
    "            \n",
    "        self.patch_size = patch_size\n",
    "        self.linear_projection = nn.Linear(in_features=d,\n",
    "                                           out_features=emb_dim)\n",
    "    \n",
    "    def _tokenize(self, x:Tensor)->Tensor:\n",
    "        input_size = x.shape[-1]\n",
    "        patches_list = []\n",
    "        for i in range(0, input_size-1, self.patch_size):\n",
    "            for j in range(0, input_size-1, self.patch_size):\n",
    "                patch = x[..., i:i+self.patch_size, j:j+self.patch_size]\n",
    "                patches_list.append(patch)\n",
    "        patches_list = torch.stack(patches_list, dim=1)\n",
    "        return patches_list\n",
    "\n",
    "    def forward(self, x: Tensor)->Tensor:\n",
    "        out = self._tokenize(x)\n",
    "        out = out.view(out.shape[0], out.shape[1], -1)\n",
    "        out = self.linear_projection(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEmbedding(nn.Module):\n",
    "    def __init__(self, batch_size, num_patches, embedded_dim):\n",
    "        super().__init__()\n",
    "        self.cls_token = nn.Parameter(torch.randn(batch_size, 1, embedded_dim))\n",
    "        self.position = nn.Parameter(torch.randn(batch_size, num_patches+1, embedded_dim))\n",
    "    \n",
    "    def forward(self, x:Tensor):\n",
    "        out = torch.cat([x, self.cls_token], dim=1)\n",
    "        out += self.position\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbeddingBlock(nn.Module):\n",
    "    def __init__(self,\n",
    "                 batch_size: int,\n",
    "                 in_channel: int,\n",
    "                 patch_size: int,\n",
    "                 num_patches: int):\n",
    "        super().__init__()\n",
    "        embedded_dim = in_channel * (patch_size ** 2)\n",
    "        self.patch_embedding = PatchEmbedding(in_channel=in_channel, patch_size=patch_size)\n",
    "        self.positional_embedding = PositionalEmbedding(batch_size=batch_size, num_patches=num_patches, embedded_dim=embedded_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.patch_embedding(x)\n",
    "        out = self.positional_embedding(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads):\n",
    "        super().__init__()\n",
    "\n",
    "        self.num_heads = num_heads\n",
    "        self.embed_dim = embed_dim\n",
    "        self.scailing_factor = (embed_dim // num_heads) ** 0.5\n",
    "\n",
    "        self.q_linear = nn.Linear(embed_dim, embed_dim)\n",
    "        self.k_linear = nn.Linear(embed_dim, embed_dim)\n",
    "        self.v_linear = nn.Linear(embed_dim, embed_dim)\n",
    "        self.fc_layer = nn.Linear(embed_dim, embed_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        queries = self.q_linear(x)\n",
    "        keys = self.k_linear(x)\n",
    "        values = self.v_linear(x)\n",
    "        \n",
    "        queries = self._split_dimension(queries)\n",
    "        keys = self._split_dimension(keys)\n",
    "        values = self._split_dimension(values)\n",
    "\n",
    "        keys.transpose_(-1, -2)\n",
    "        attention_score = torch.matmul(queries, keys)\n",
    "        attention_score /= self.scailing_factor\n",
    "\n",
    "        attention_weight = F.softmax(attention_score, dim=-1)\n",
    "        attention = torch.matmul(attention_weight, values)\n",
    "        attention = attention.transpose(1, 2).contiguous()\n",
    "        batch_size, num_patches, *_  = attention.shape\n",
    "        attention = attention.view(batch_size, num_patches, self.embed_dim)\n",
    "        out = self.fc_layer(attention)\n",
    "        return out\n",
    "        \n",
    "    def _split_dimension(self, x: Tensor):\n",
    "        batch_size, num_patches, embed_dim = x.shape\n",
    "        x = x.view(batch_size, num_patches, self.num_heads, embed_dim//self.num_heads)\n",
    "        x.transpose_(1, 2)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForwardBlock(nn.Sequential):\n",
    "    def __init__(self, embed_dim, expansion=4, drop_out=0.1):\n",
    "        super().__init__(\n",
    "            nn.Linear(embed_dim, embed_dim*expansion),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(drop_out),\n",
    "            nn.Linear(embed_dim*expansion, embed_dim),\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderBlock(nn.Module):\n",
    "    def __init__(self,\n",
    "                 embed_dim,\n",
    "                 num_heads,\n",
    "                 ):\n",
    "        super().__init__()\n",
    "        self.lm_layer1 = nn.LayerNorm(embed_dim)\n",
    "        self.msa_block = MultiHeadAttention(embed_dim, num_heads)\n",
    "        self.lm_layer2 = nn.LayerNorm(embed_dim)\n",
    "        self.mlp_block = FeedForwardBlock(embed_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        iden = x\n",
    "        out = self.lm_layer1(x)\n",
    "        out += iden\n",
    "        iden = out\n",
    "        out = self.mlp_block(out)\n",
    "        out += iden\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self,\n",
    "                 embed_dim,\n",
    "                 num_heads,\n",
    "                 num_repeats,\n",
    "                 ):\n",
    "        super().__init__()\n",
    "        self.blocks = [EncoderBlock(embed_dim, num_heads) for _ in range(num_repeats)]\n",
    "        self.blocks = nn.ModuleList(self.blocks)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = x\n",
    "        for block in self.blocks:\n",
    "            out = block(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_block = EmbeddingBlock(4, 3, 16, 196)\n",
    "input_image = torch.randn(4, 3, 224, 224)\n",
    "out = embedding_block(input_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 197, 768])"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = Encoder(768, 8, 16)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 197, 768])"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder(out).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
